# Quick Start Guide - Real Model Inference

Get up and running with real LLM models in nano-vllm-go!

## ğŸš€ Choose Your Mode

You have **3 options** for running real models. Pick based on your needs:

| Mode | Setup | Speed | Best For |
|------|-------|-------|----------|
| **HTTP** | 5 min | 40-80 tok/s | Development, quick testing |
| **ONNX** | 15 min | 20-40 tok/s | Production, single binary |
| **Native Go** | 5 min | 5-10 tok/s | Learning, no dependencies |

## ğŸ”¥ Option 1: HTTP Backend (Fastest Setup)

**What:** Go scheduler + Python server for inference
**Setup:** 5 minutes
**Speed:** 40-80 tokens/second

```bash
# One command does everything!
./scripts/start.sh http "What is the capital of France?"

# Or just:
./scripts/start_http.sh "Your question here"
```

**How it works:**
- Python server loads HuggingFace model (auto-downloads first time)
- Go client handles scheduling and batching
- Communicates via HTTP

**Use when:**
- You want to test quickly
- You're okay with Python dependency
- You're developing/experimenting

## âš¡ Option 2: ONNX Runtime (Production Ready)

**What:** Pure Go with ONNX model files
**Setup:** 15 minutes (one-time model export)
**Speed:** 20-40 tokens/second

```bash
# One command does everything!
./scripts/start.sh onnx "What is AI?"

# Or just:
./scripts/start_onnx.sh "Your question here"
```

**How it works:**
- Export HuggingFace model to ONNX format (one-time)
- Go code loads ONNX model directly
- Single binary deployment

**Use when:**
- You want production deployment
- You want single binary
- You don't want Python at runtime

## ğŸ“ Option 3: Pure Go Transformer (Educational)

**What:** Complete transformer implemented in Go from scratch
**Setup:** 5 minutes
**Speed:** 5-10 tokens/second

```bash
# One command does everything!
./scripts/start.sh native "Once upon a time"

# Or just:
./scripts/start_native.sh "Your prompt here"
```

**How it works:**
- Every operation (matmul, attention, etc.) in pure Go
- Loads GPT-2 weights
- Zero runtime dependencies

**Use when:**
- You want to learn how transformers work
- You want to modify the architecture
- You want zero dependencies
- Speed doesn't matter

## ğŸ“‹ What Each Script Does

### start_http.sh
1. Checks/installs Python dependencies
2. Builds Go HTTP client
3. Starts Python server (if not running)
4. Runs your prompt
5. Keeps server running for more requests

### start_onnx.sh
1. Checks if ONNX model exists
2. If not: exports HuggingFace model to ONNX (one-time)
3. Builds Go ONNX runner
4. Runs your prompt

### start_native.sh
1. Checks if GPT-2 model downloaded
2. If not: downloads GPT-2 safetensors (one-time)
3. Builds Go native transformer
4. Runs your prompt

### start.sh (Master Script)
- Shows comparison table
- Dispatches to appropriate mode
- Provides help and examples

## ğŸ¯ Examples

### Quick Test (HTTP - Fastest)
```bash
./scripts/start.sh http "What is machine learning?"
```

### Production Test (ONNX)
```bash
# First time: exports model (~10 min)
# Subsequent: instant start
./scripts/start.sh onnx "Explain quantum computing"
```

### Learn Transformers (Pure Go)
```bash
./scripts/start.sh native "The meaning of life is"
```

### Custom Models

**HTTP mode with different model:**
```bash
MODEL_NAME='TinyLlama/TinyLlama-1.1B-Chat-v1.0' \
  ./scripts/start_http.sh "Hello world"
```

**ONNX mode with different model:**
```bash
MODEL_NAME='microsoft/phi-2' \
  ./scripts/start_onnx.sh "Hello world"
```

### Multiple Questions

All modes support multiple prompts:
```bash
./scripts/start.sh http \
  "What is AI?" \
  "What is ML?" \
  "What is DL?"
```

## ğŸ› ï¸ Manual Setup

If you prefer step-by-step control:

### HTTP Backend
```bash
# Terminal 1: Start server
python3 server.py

# Terminal 2: Run client
go build -o bin/http_test ./purego/example_http
./bin/http_test "Your question"
```

### ONNX Runtime
```bash
# One-time: Export model
python3 scripts/export_to_onnx.py \
  --model Qwen/Qwen2-0.5B-Instruct \
  --output ./models/qwen2-onnx

# Build and run
go build -o bin/onnx_test ./purego/example_onnx
MODEL_CONFIG=./models/qwen2-onnx/nano_config.json \
  ./bin/onnx_test "Your question"
```

### Pure Go Transformer
```bash
# One-time: Download GPT-2
python3 scripts/download_gpt2.py \
  --model gpt2 \
  --output ./models/gpt2

# Build and run
go build -o bin/native_test ./purego/example_native
MODEL_PATH=./models/gpt2/model.safetensors \
TOKENIZER_PATH=./models/gpt2 \
  ./bin/native_test "Your prompt"
```

## ğŸ“Š Performance Comparison

### HTTP Backend
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metric       â”‚ Value       â”‚ Notes       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Setup        â”‚ 5 min       â”‚ First time  â”‚
â”‚ Prefill      â”‚ 100 tok/s   â”‚ Depends     â”‚
â”‚ Decode       â”‚ 40-80 tok/s â”‚ on model    â”‚
â”‚ Memory       â”‚ 600MB-2GB   â”‚             â”‚
â”‚ Dependencies â”‚ Python      â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ONNX Runtime
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metric       â”‚ Value       â”‚ Notes       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Setup        â”‚ 15 min      â”‚ First time  â”‚
â”‚ Prefill      â”‚ 50 tok/s    â”‚ CPU only    â”‚
â”‚ Decode       â”‚ 20-40 tok/s â”‚             â”‚
â”‚ Memory       â”‚ 600MB-2GB   â”‚             â”‚
â”‚ Dependencies â”‚ None        â”‚ Runtime     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pure Go
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metric       â”‚ Value       â”‚ Notes       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Setup        â”‚ 5 min       â”‚ First time  â”‚
â”‚ Prefill      â”‚ 10-20 tok/s â”‚ Educational â”‚
â”‚ Decode       â”‚ 5-10 tok/s  â”‚             â”‚
â”‚ Memory       â”‚ 500MB       â”‚ GPT-2 small â”‚
â”‚ Dependencies â”‚ None        â”‚ Zero!       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¨ Architecture Overview

### HTTP Mode
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Go Client   â”‚  HTTP   â”‚ Python Serverâ”‚
â”‚             â”‚ <-----> â”‚              â”‚
â”‚ â€¢ Scheduler â”‚         â”‚ â€¢ PyTorch    â”‚
â”‚ â€¢ Batching  â”‚         â”‚ â€¢ HF Models  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ONNX Mode
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Single Go Process      â”‚
â”‚                          â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ nano-vllm Scheduler  â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚            â”‚             â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ ONNX Runtime (CGo)   â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pure Go Mode
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Single Go Process      â”‚
â”‚                          â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ nano-vllm Scheduler  â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚            â”‚             â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Pure Go Transformer  â”‚ â”‚
â”‚ â”‚ â€¢ MatMul             â”‚ â”‚
â”‚ â”‚ â€¢ Attention          â”‚ â”‚
â”‚ â”‚ â€¢ All ops in Go!     â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› Troubleshooting

### HTTP Mode

**Server won't start:**
```bash
# Check Python version
python3 --version  # Need 3.8+

# Install dependencies
pip3 install flask torch transformers

# Check logs
tail -f /tmp/nano-vllm-server.log
```

**Connection refused:**
```bash
# Check if server is running
curl http://localhost:8000/health

# Restart server
kill $(cat /tmp/nano-vllm-server.pid)
./scripts/start_http.sh
```

### ONNX Mode

**Model export fails:**
```bash
# Try smaller model
MODEL_NAME='Qwen/Qwen2-0.5B-Instruct' ./scripts/start_onnx.sh

# Check Python packages
pip3 install --upgrade torch transformers onnx
```

**ONNX Runtime errors:**
```bash
# The Go package should auto-download ONNX Runtime
# If it fails, check: https://onnxruntime.ai/

# On macOS:
brew install onnxruntime
```

### Pure Go Mode

**Download fails:**
```bash
# Manual download
python3 scripts/download_gpt2.py \
  --model gpt2 \
  --output ./models/gpt2

# Try smaller model if out of memory
# (gpt2 is already the smallest at 124M params)
```

## ğŸ“š Learn More

- **HTTP Backend**: [TEST_REAL_MODEL.md](TEST_REAL_MODEL.md)
- **ONNX Runtime**: [ONNX_GUIDE.md](ONNX_GUIDE.md)
- **Pure Go**: [NATIVE_TRANSFORMER_GUIDE.md](NATIVE_TRANSFORMER_GUIDE.md)
- **Architecture**: [ARCHITECTURE.md](ARCHITECTURE.md)
- **API Reference**: [README.md](README.md)

## ğŸ’¡ Recommendations

**Starting out?** â†’ Use HTTP mode (fastest setup)
```bash
./scripts/start.sh http "Hello world"
```

**Going to production?** â†’ Use ONNX mode (single binary)
```bash
./scripts/start.sh onnx "Hello world"
```

**Want to learn?** â†’ Use Pure Go mode (see all the code)
```bash
./scripts/start.sh native "Hello world"
```

**Not sure?** â†’ Try all three!
```bash
./scripts/start.sh http "What is AI?"
./scripts/start.sh onnx "What is AI?"
./scripts/start.sh native "What is AI?"
```

## ğŸ‰ Summary

**You have 3 ways to run real LLM models:**

1. **HTTP** - Python server + Go client (fastest setup)
2. **ONNX** - Pure Go with ONNX models (production ready)
3. **Native** - Pure Go transformer (educational)

**All modes:**
- âœ… Use nano-vllm-go scheduler
- âœ… Support continuous batching
- âœ… Handle multiple prompts
- âœ… Work with real models
- âœ… Have one-command startup scripts

**Choose based on:**
- Speed requirements
- Deployment constraints
- Learning goals

**Get started now:**
```bash
./scripts/start.sh
```

Happy inferencing! ğŸš€
