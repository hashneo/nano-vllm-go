# Pure Go Transformer - Complete Guide

## What Is This?

A **100% pure Go implementation** of a GPT-2 transformer. No Python, no ONNX, no external ML libraries - just Go code that implements the actual neural network!

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Pure Go Implementation                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  nano-vllm-go (Scheduler)                â”‚ â”‚
â”‚  â”‚  â€¢ Continuous batching                   â”‚ â”‚
â”‚  â”‚  â€¢ Memory management                     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚             â”‚                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  NativeModelRunner                       â”‚ â”‚
â”‚  â”‚  â€¢ Pure Go inference                     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚             â”‚                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Transformer Layers (Pure Go)            â”‚ â”‚
â”‚  â”‚                                          â”‚ â”‚
â”‚  â”‚  â€¢ Embedding Lookup                      â”‚ â”‚
â”‚  â”‚  â€¢ Multi-Head Attention                  â”‚ â”‚
â”‚  â”‚  â€¢ Feed-Forward Network                  â”‚ â”‚
â”‚  â”‚  â€¢ Layer Normalization                   â”‚ â”‚
â”‚  â”‚  â€¢ GELU Activation                       â”‚ â”‚
â”‚  â”‚                                          â”‚ â”‚
â”‚  â”‚  All implemented as Go functions!        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## What Was Implemented

### Core Tensor Operations (~250 lines)
**File: `purego/tensor/tensor.go`**

- `MatMul(a, b)` - Matrix multiplication
- `Add(a, b)` - Element-wise addition
- `Transpose(t)` - Matrix transpose
- `Softmax(t)` - Softmax activation
- `GELU(t)` - GELU activation function
- `LayerNorm(t, weight, bias)` - Layer normalization
- `Scale(t, factor)` - Scalar multiplication
- Tensor reshape, slice operations

### Multi-Head Attention (~170 lines)
**File: `purego/tensor/attention.go`**

```go
type MultiHeadAttention struct {
    NumHeads  int
    HeadDim   int
    QWeight   *Tensor  // Query projection
    KWeight   *Tensor  // Key projection
    VWeight   *Tensor  // Value projection
    OutWeight *Tensor  // Output projection
}
```

Implements:
- Q, K, V projections
- Scaled dot-product attention
- Causal masking (for autoregressive generation)
- Multi-head splitting and combining

### Transformer Block (~90 lines)
**File: `purego/tensor/transformer.go`**

```go
type TransformerBlock struct {
    Attention *MultiHeadAttention
    FFN       *FeedForward
    LN1       *LayerNormLayer
    LN2       *LayerNormLayer
}
```

Implements:
- Self-attention with residual connection
- Feed-forward network with residual
- Layer normalization before each component

### GPT-2 Model (~130 lines)
**File: `purego/tensor/model.go`**

```go
type GPT2Model struct {
    TokenEmbedding *Tensor
    PosEmbedding   *Tensor
    Blocks         []*TransformerBlock
    LNFinal        *LayerNormLayer
    LMHead         *Tensor
}
```

Full GPT-2 architecture:
- Token + position embeddings
- 12 transformer layers
- Final layer normalization
- Language modeling head

### Weight Loader (~220 lines)
**File: `purego/tensor/loader.go`**

Loads GPT-2 weights from safetensors format:
- Parses safetensors binary format
- Converts F32/F16 to Go float32
- Maps weights to model components
- Handles GPT-2 specific weight layout

### ModelRunner Integration (~140 lines)
**File: `purego/native_runner.go`**

Integrates with nano-vllm-go:
- Implements `ModelRunner` interface
- Batches sequences
- Temperature sampling
- Seamlessly works with scheduler

## Quick Start

### 1. Install Dependencies

```bash
# Python (one-time, for downloading model)
pip install torch transformers safetensors
```

### 2. Download GPT-2

```bash
cd ~/Development/github/nano-vllm-go

# Download GPT-2 small (smallest, ~500MB)
python3 scripts/download_gpt2.py --model gpt2 --output ./models/gpt2

# Or other sizes:
# gpt2-medium (~1.5GB)
# gpt2-large (~3GB)
# gpt2-xl (~6GB)
```

This creates:
```
models/gpt2/
â”œâ”€â”€ model.safetensors    # Model weights
â”œâ”€â”€ vocab.json           # Vocabulary
â”œâ”€â”€ tokenizer.json       # Tokenizer config
â””â”€â”€ model_info.json      # Model metadata
```

### 3. Build and Run

```bash
# Build
go build -o bin/native_test ./purego/example_native

# Run
export MODEL_PATH=./models/gpt2/model.safetensors
export TOKENIZER_PATH=./models/gpt2
./bin/native_test "Once upon a time"
```

## Expected Output

```
Nano-vLLM-Go - Pure Go Transformer
===================================

Loading model from: ./models/gpt2/model.safetensors
Loading tokenizer from: ./models/gpt2

Loading pure Go transformer...
âœ“ Loaded GPT-2 weights from ./models/gpt2/model.safetensors
GPT-2 Model Configuration:
  Vocabulary: 50257
  Hidden size: 768
  Layers: 12
  Heads: 12
  FFN dim: 3072
  Max seq len: 1024
  Parameters: ~124.4M

Loading tokenizer...
âœ“ Loaded GPT-2 tokenizer (vocab: 50257)

Generating text for 1 prompt(s)...

Generating [Prefill: 12tok/s, Decode: 5tok/s] 100% [====] (1/1, 0.2 it/s)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
RESULTS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ Prompt 1: Once upon a time
ğŸ’¬ Generated: Once upon a time, there was a king who lived in a castle...
ğŸ“Š Tokens: 20

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ“ Generation complete!

Note: This is a PURE GO transformer implementation!
No Python, no ONNX, no external libraries - just Go!
```

## How It Works

### 1. Token Embedding

```go
// Input: [1, 2, 3, 4]  (token IDs)
// Output: [1, 4, 768]  (embedded vectors)

for i, tokenID := range tokenIDs {
    for j := 0; j < hidden; j++ {
        result[i][j] = TokenEmbedding[tokenID][j] + PosEmbedding[i][j]
    }
}
```

### 2. Multi-Head Attention

```go
// Q, K, V projections
Q = input @ QWeight  // [seq, hidden] @ [hidden, hidden]
K = input @ KWeight
V = input @ VWeight

// Split into heads
Q = reshape(Q, [seq, num_heads, head_dim])
K = reshape(K, [seq, num_heads, head_dim])
V = reshape(V, [seq, num_heads, head_dim])

// Attention scores
scores = Q @ K^T / sqrt(head_dim)
scores = apply_causal_mask(scores)
attn_weights = softmax(scores)

// Apply to values
output = attn_weights @ V
output = reshape(output, [seq, hidden])
```

### 3. Feed-Forward Network

```go
// Two linear layers with GELU
x = input @ W1 + B1      // [seq, hidden] @ [hidden, ffn_dim]
x = GELU(x)              // Activation
x = x @ W2 + B2          // [seq, ffn_dim] @ [ffn_dim, hidden]
```

### 4. Complete Forward Pass

```go
x = embed(tokens)           // Token + position embeddings

for each transformer_block:
    // Self-attention block
    residual = x
    x = layer_norm(x)
    x = multi_head_attention(x)
    x = x + residual

    // Feed-forward block
    residual = x
    x = layer_norm(x)
    x = feed_forward(x)
    x = x + residual

x = layer_norm(x)          // Final norm
logits = x @ LM_head       // Project to vocabulary
return logits[-1]          // Return last token logits
```

## Performance

### Pure Go (CPU)

**GPT-2 Small (124M params):**
- Prefill: 10-20 tok/s (depends on prompt length)
- Decode: 3-8 tok/s (single token generation)
- Memory: ~500MB (model weights)

**Compared to other implementations:**

| Implementation | Speed | Setup | Dependencies |
|---------------|-------|-------|-------------|
| Pure Go | 5-10 tok/s | Easy | None |
| ONNX Runtime | 20-40 tok/s | Medium | ONNX lib |
| PyTorch CPU | 50-100 tok/s | Hard | Python |
| PyTorch GPU | 1000+ tok/s | Hard | Python + CUDA |

### Why So Slow?

1. **No SIMD** - Not using CPU vector instructions
2. **No BLAS** - Naive matrix multiplication (O(nÂ³))
3. **No GPU** - Running on CPU only
4. **Go overhead** - Bounds checking, garbage collection

### How to Make It Faster?

**Easy improvements (2-3x):**
- Use `gonum/blas` for matrix operations
- Pre-allocate buffers (reduce GC)
- Cache frequently used values

**Medium improvements (5-10x):**
- CGo to Intel MKL or OpenBLAS
- SIMD intrinsics for key operations
- Goroutines for parallel attention heads

**Hard improvements (20-50x):**
- GPU implementation via CGo to CUDA
- Quantization (INT8/FP16)
- Kernel fusion (combine operations)

## When to Use This?

### âœ… USE when you want to:
- **Learn** - Understand transformers deeply
- **Experiment** - Modify architecture easily
- **Teach** - Show how transformers work
- **Embed** - Include in Go application without dependencies
- **Edge devices** - Deploy where Python isn't available
- **Security** - Audit all code (no binary blobs)
- **Fun** - Build cool stuff in pure Go!

### âŒ DON'T USE when you need:
- **Production speed** - Use ONNX or PyTorch
- **Large models** - 7B+ models too slow
- **GPU acceleration** - This is CPU only
- **Latest models** - Only GPT-2 architecture

## Code Examples

### Basic Usage

```go
package main

import (
    "nano-vllm-go/nanovllm"
    "nano-vllm-go/purego"
)

func main() {
    // Load model
    config := nanovllm.NewConfig(".", nanovllm.WithEOS(50256))
    runner, _ := purego.NewNativeModelRunner("./models/gpt2/model.safetensors", config)
    tokenizer, _ := purego.NewGPT2Tokenizer("./models/gpt2")
    llm := nanovllm.NewLLMWithComponents(config, runner, tokenizer)

    // Generate
    samplingParams := nanovllm.NewSamplingParams(
        nanovllm.WithTemperature(0.8),
        nanovllm.WithMaxTokens(50),
    )
    outputs, _ := llm.GenerateSimple([]string{"Hello world"}, samplingParams, false)

    println(outputs[0].Text)
}
```

### Custom Sampling

```go
// Greedy (deterministic)
sampling := nanovllm.NewSamplingParams(
    nanovllm.WithTemperature(0.0),
)

// Creative (random)
sampling := nanovllm.NewSamplingParams(
    nanovllm.WithTemperature(1.2),
)

// Nucleus sampling
sampling := nanovllm.NewSamplingParams(
    nanovllm.WithTemperature(0.8),
    nanovllm.WithTopP(0.9),
)
```

### Multiple Prompts (Batching)

```go
prompts := []string{
    "The meaning of life is",
    "Once upon a time",
    "In a galaxy far away",
}

outputs, _ := llm.GenerateSimple(prompts, samplingParams, true)

for i, output := range outputs {
    fmt.Printf("%s -> %s\n", prompts[i], output.Text)
}
```

## Implementation Details

### Tensor Layout

Tensors are stored in row-major order:
```go
// 2D tensor [2, 3]:
// [[1, 2, 3],
//  [4, 5, 6]]
//
// Stored as: [1, 2, 3, 4, 5, 6]
// Access: data[i*cols + j]
```

### Matrix Multiplication

```go
// C = A @ B
// A: [m, k], B: [k, n] -> C: [m, n]
for i := 0; i < m; i++ {
    for j := 0; j < n; j++ {
        sum := 0
        for p := 0; p < k; p++ {
            sum += A[i*k + p] * B[p*n + j]
        }
        C[i*n + j] = sum
    }
}
```

### Attention Masking

```go
// Causal mask prevents attending to future positions
for i := 0; i < seq_len; i++ {
    for j := i+1; j < seq_len; j++ {
        scores[i*seq_len + j] = -1e10  // Effectively -infinity
    }
}
```

## File Structure

```
purego/tensor/
â”œâ”€â”€ tensor.go         # Basic tensor operations (250 lines)
â”œâ”€â”€ attention.go      # Multi-head attention (170 lines)
â”œâ”€â”€ transformer.go    # Transformer blocks (90 lines)
â”œâ”€â”€ model.go          # GPT-2 model (130 lines)
â””â”€â”€ loader.go         # Weight loading (220 lines)

purego/
â”œâ”€â”€ native_runner.go  # ModelRunner integration (140 lines)
â””â”€â”€ gpt2_tokenizer.go # GPT-2 tokenizer (180 lines)

Total: ~1,180 lines of pure Go
```

## Troubleshooting

### Model won't load

```bash
# Check file exists
ls -lh ./models/gpt2/model.safetensors

# Re-download
python3 scripts/download_gpt2.py --model gpt2 --output ./models/gpt2
```

### Out of memory

```bash
# Use smaller batch size
config := nanovllm.NewConfig(".",
    nanovllm.WithMaxNumSeqs(4),  # Reduce from 8
)

# Or use smaller max tokens
samplingParams := nanovllm.NewSamplingParams(
    nanovllm.WithMaxTokens(20),  # Reduce from 50
)
```

### Too slow

This is expected! Pure Go transformers are educational, not for production.

For faster inference:
1. Use ONNX implementation (5-10x faster)
2. Use PyTorch backend (10-20x faster)
3. Wait for optimized version with BLAS

## Next Steps

### Optimize Performance
- Add gonum/blas for matrix ops
- Use CGo to call MKL/OpenBLAS
- Implement KV caching properly
- Add FP16 support

### Add Features
- Support more architectures (Llama, Mistral)
- Implement beam search
- Add quantization (INT8)
- Batched inference

### Educational
- Add visualization of attention weights
- Step-by-step debugging mode
- Interactive notebook integration

## Summary

**What you get:**
- âœ… 100% pure Go transformer
- âœ… Real neural network (GPT-2)
- âœ… Educational code
- âœ… No dependencies
- âœ… Complete integration with nano-vllm-go

**Performance:**
- CPU: 5-10 tok/s (educational speed)
- Memory: ~500MB
- Good for learning, experimenting

**When to use:**
- Learning how transformers work
- Embedding in Go apps without deps
- Edge deployment without Python
- Fun projects!

**Try it:**
```bash
# Download model
python3 scripts/download_gpt2.py

# Run
go build -o bin/native_test ./purego/example_native
MODEL_PATH=./models/gpt2/model.safetensors ./bin/native_test "Hello world"
```

You now have a **real, working transformer in pure Go**! ğŸš€
